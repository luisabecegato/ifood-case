# -*- coding: utf-8 -*-
"""1_data_processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/127pSiHW_dw641V4Agz1keMej2AWWLA6A
"""

# Instala Java
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

# Baixa e extrai Spark
!curl -L -o spark.tgz https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz
!tar -xzf spark.tgz

# Instala findspark
!pip install -q findspark

# Configura variáveis de ambiente
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.3.2-bin-hadoop3"

import findspark
findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("iFood Case - Data Processing") \
    .getOrCreate()

offers_df = spark.read.json("offers.json")
profile_df = spark.read.json("profile.json")
transactions_df = spark.read.json("transactions.json")

offers_df.printSchema()
profile_df.printSchema()
transactions_df.printSchema()

'''
offers_df.show(5, truncate=False)
profile_df.show(5, truncate=False)
transactions_df.show(5, truncate=False)
'''

"""Esse erro do transactions_df.show() do PySpark indica que o arquivo .json tem linhas corrompidas ou malformadas.
Por segurança, ele joga essas linhas problemáticas na coluna chamada "_corrupt_record".
Vou ler o arquivo em modo permissivo a seguir e ver onde estão os erros

"""

transactions_df = spark.read \
    .option("mode", "PERMISSIVE") \
    .option("columnNameOfCorruptRecord", "_corrupt_record") \
    .json("transactions.json")

transactions_df.show(5, truncate=False)

#transactions_df.filter(transactions_df["_corrupt_record"].isNotNull()).count()

transactions_df.select("event").distinct().show()

trans_df = transactions_df.filter(transactions_df["event"] == "transaction")
trans_df.show(5, truncate=False)

received_df = transactions_df.filter(transactions_df["event"] == "offer received")
received_df.show(5, truncate=False)

viewed_df = transactions_df.filter(transactions_df["event"] == "offer viewed")
viewed_df.show(5, truncate=False)

completed_df = transactions_df.filter(transactions_df["event"] == "offer completed")
completed_df.show(5, truncate=False)

received_df = received_df.withColumn("offer_id", received_df["value"]["offer id"])

viewed_df = viewed_df.withColumn("offer_id", viewed_df["value"]["offer id"])

completed_df = completed_df.withColumn("offer_id", completed_df["value"]["offer_id"]) \
                           .withColumn("reward", completed_df["value"]["reward"])

trans_df = trans_df.withColumn("amount", trans_df["value"]["amount"])

base_df = received_df.select(
    "account_id",
    "offer_id",
    "time_since_test_start"
).withColumnRenamed("time_since_test_start", "received_time")

viewed_df = viewed_df.select(
    "account_id",
    "offer_id",
    "time_since_test_start"
).withColumnRenamed("time_since_test_start", "viewed_time")

base_df = base_df.join(viewed_df, ["account_id", "offer_id"], how="left")

completed_df = completed_df.select(
    "account_id",
    "offer_id",
    "time_since_test_start",
    "reward"
).withColumnRenamed("time_since_test_start", "completed_time")

base_df = base_df.join(completed_df, ["account_id", "offer_id"], how="left")

from pyspark.sql.functions import when

base_df = base_df.withColumn("foi_visualizada", when(base_df["viewed_time"].isNotNull(), 1).otherwise(0))
base_df = base_df.withColumn("foi_completada", when(base_df["completed_time"].isNotNull(), 1).otherwise(0))

from pyspark.sql.functions import round

base_df = base_df.withColumn("tempo_ate_visualizacao", round(base_df["viewed_time"] - base_df["received_time"], 2))
base_df = base_df.withColumn("tempo_ate_completamento", round(base_df["completed_time"] - base_df["received_time"], 2))

base_df.select(
    "account_id", "offer_id",
    "received_time", "viewed_time", "completed_time",
    "foi_visualizada", "foi_completada",
    "tempo_ate_visualizacao", "tempo_ate_completamento"
).show(10, truncate=False)

# Renomeia id para evitar conflito com offer_id
profile_df = profile_df.withColumnRenamed("id", "account_id")

base_df = base_df.join(profile_df, on="account_id", how="left")

# Renomeia id para evitar conflito com account_id
offers_df = offers_df.withColumnRenamed("id", "offer_id")

base_df = base_df.join(offers_df, on="offer_id", how="left")

# Tratamento de nulos e inconsistências
base_df = base_df.filter(
    base_df["age"].isNotNull() &
    base_df["gender"].isin("F", "M") &
    base_df["credit_card_limit"].isNotNull() &
    base_df["offer_type"].isNotNull() &
    (base_df["age"] > 10) & (base_df["age"] < 100)
)

# Normalização do limite
from pyspark.sql.functions import min as spark_min, max as spark_max, col

min_lim = base_df.agg(spark_min("credit_card_limit")).first()[0]
max_lim = base_df.agg(spark_max("credit_card_limit")).first()[0]

base_df = base_df.withColumn(
    "credit_card_limit_norm",
    (col("credit_card_limit") - min_lim) / (max_lim - min_lim)
)


base_df.toPandas().to_csv("base_df.csv", index=False)